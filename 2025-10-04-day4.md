# Journal — 2025-09-13 — EXAMPLE DAY

## 1) What I learned (bullets, not prose)
- Data quality (DQ) has multiple dimensions: accuracy, completeness, reliability, relevancy, and timeliness.
- DQ happens in layers of the data stack — basic checks in clean, business-specific checks in mart.
- Every time data is loaded, basic checks should be applied.
- DQ contracts ensure alignment with business expectations.
- Dashboards should display DQ metrics clearly: freshness, nulls, test pass rate, etc.
- Roles matter: data owners decide thresholds; data stewards enforce them.
- Governance ensures quality stays stable despite changes in pipelines or data.

## 2) New vocabulary (define in your own words)
- Data Quality (DQ) — Ensuring data is accurate, complete, timely, and usable for its purpose.
- Threshold — A limit or rule (e.g. “<5% nulls allowed”) that determines whether a DQ test passes or fails.
- Data Steward — The person responsible for executing data quality practices and fixing issues.
- Data Owner — The person who defines DQ expectations and approves data use.
- Freshness Lag — The delay between when data was expected vs. when it was actually loaded.
- Range Check — A test to verify if a number is within a valid range (e.g. score between 0 and 100).
- Category Check — A validation that values fall into an allowed set (e.g. gender in ('M', 'F')).
- Null Check — Ensures that required fields aren’t blank or missing.
- Referential Integrity — Making sure foreign keys (e.g. id_assessment) exist in their parent table.
- Volume Check — Monitoring row counts over time to detect data loss or duplication.
- Baseline — The “normal” value used for comparison in tests like volume or freshness checks.

## 3) Data Engineering mindset applied (what principles did I use?)
- Quality is best managed through layered validation, basic in clean, business rules in mart.
- Test automation and documentation in dbt help maintain clarity and reliability.
- Trust is built by making problems visible, even partial failure is better than hidden issues.


## 4) Open questions (things I still don’t get)
- How do i differentiate acceptable anomalies vs. real data issues?


## 5) Next actions (small, doable steps)
- Finish implementing data quality tests for the OULAD dataset.
- Start the Instacart assignment: review dataset schema, define facts/dims, and plan DQ checks


### Mini reflection (3–5 sentences)
I used to think data quality was just about catching errors, but now I see it’s really about building trust in the data. I learned how different types of checks belong in different layers, with basic validations in clean and more business-specific ones in mart. It also stood out to me that thresholds shouldn’t just be guessed as it requires collaboration with the business. Also, I’ve come to understand that data quality is something you maintain continuously, not just something you check once and forget.


BONUS: What is a meme that best describes what you feel or your learning today?
<img width="1179" height="1412" alt="image" src="https://github.com/user-attachments/assets/acad7112-b617-4005-893a-92713bdf43c2" />




### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](../assets/meme.png "what is a data engineer?")
